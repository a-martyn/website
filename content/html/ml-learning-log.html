<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="../css/markdown.css" type="text/css" />
</head>
<body>
<div class="sidebar">
<a class="active" href="#home">
<h1>
Alan Martyn
</h1>
<p></a> <a href="https://www.alanmartyn.com/ml-curriculum.html" target="targetframe">Machine Learning: Curriculum</a> <a href="https://www.alanmartyn.com/ml-learning-log.html" target="targetframe">Machine Learning: Log</a></p>
</div>
<div class="content">
<h2 id="machine-learning-log">Machine Learning: Log</h2>
<p><em>Day-by-day notes as I study machine learning.</em></p>
<h3 id="section">66. 2018-10-23</h3>
<p>Introduction to Statistical Learning: ch8: <strong>Tree-Based Methods</strong> Applied:</p>
<ul>
<li>Comparison of tree based methods with KNN, regression and the lasso.</li>
<li><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch8_tree_based_methods_applied.ipynb">Notebook: ch8_tree_based_methods_applied.ipynbb</a></li>
</ul>
<h3 id="section-1">65. 2018-10-22</h3>
<p>Introduction to Statistical Learning: ch8: <strong>Tree-Based Methods</strong> Applied:</p>
<ul>
<li>Application of tree based methods – including Tree Pruning, Bagging, Random Forest, Boosting – to various datasets</li>
<li><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch8_tree_based_methods_applied.ipynb">Notebook: ch8_tree_based_methods_applied.ipynbb</a></li>
</ul>
<h3 id="section-2">64. 2018-10-19</h3>
<p>Introduction to Statistical Learning: ch8: <strong>Tree-Based Methods</strong> Labs:</p>
<ul>
<li>Review of tree based methods including Tree Pruning, Bagging, Random Forest, Boosting</li>
<li><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch8_tree_based_methods_labs.ipynb">Notebook: ch8_tree_based_methods_labs.ipynb</a></li>
</ul>
<h3 id="section-3">63. 2018-10-18</h3>
<p>Introduction to Statistical Learning: ch7: <strong>Moving Beyond Linearity</strong> Applied:</p>
<ul>
<li><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch7_moving_beyond_linearity_applied.ipynb">Notebook: ch7_moving_beyond_linearity_applied.ipynb</a></li>
</ul>
<h3 id="section-4">62. 2018-10-17</h3>
<p>Introduction to Statistical Learning: ch7: <strong>Moving Beyond Linearity</strong> Applied:</p>
<ul>
<li>GAMs</li>
<li>implemented backfitting algorithm from scratch</li>
<li>comparison of backfitting and multivariate OLS in simulated setting</li>
<li><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch7_moving_beyond_linearity_applied.ipynb">Notebook: ch7_moving_beyond_linearity_applied.ipynb</a></li>
</ul>
<h3 id="section-5">61. 2018-10-16</h3>
<p>Introduction to Statistical Learning: ch7: <strong>Moving Beyond Linearity</strong> Applied:</p>
<ul>
<li>Analysis of Variance (ANOVA)</li>
<li>Comparison of Anova with Cross-validation for model selection</li>
<li>polynomial regression</li>
<li>step function regression</li>
<li>cubic spline regression</li>
<li>natural spline regression</li>
<li><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch7_moving_beyond_linearity_applied.ipynb">Notebook: ch7_moving_beyond_linearity_applied.ipynb</a></li>
</ul>
<h3 id="section-6">60. 2018-10-15</h3>
<p>Introduction to Statistical Learning: ch7: <strong>Moving Beyond Linearity</strong> Lab:</p>
<ul>
<li>Ch7 Lab</li>
<li>univariate polynomial regression</li>
<li>analytic confidence intervals</li>
<li>bootstrapped confidence intervals</li>
<li>comparison of splines in univariate setting</li>
<li>comparison of GAM configurations with ANOVA</li>
<li><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch7_moving_beyond_linearity_labs.ipynb">Notebook: ch7_moving_beyond_linearity_labs.ipynb</a></li>
</ul>
<h3 id="section-7">59. 2018-10-12</h3>
<p>Introduction to Statistical Learning: ch6: <strong>Linear Model Selection and Regularization</strong> Applied:</p>
<ul>
<li>Analysis to compare linear models</li>
<li>Ch7 Reading</li>
<li><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch6_linear_model_selection_and_regularisation_applied.ipynb">Notebook: ch6_linear_model_selection_and_regularisation_applied.ipynb</a></li>
</ul>
<h3 id="section-8">58. 2018-10-11</h3>
<p>Introduction to Statistical Learning: ch6: <strong>Linear Model Selection and Regularization</strong> Applied:</p>
<ul>
<li>detailed comparison of model selection techniques in context <strong>real dataset</strong> to predict the number of applications received by U.S. colleges. Techniques considered:</li>
<li>lasso</li>
<li>ridge regression</li>
<li>backward stepwise</li>
<li>PCR</li>
<li>PLS</li>
<li><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch6_linear_model_selection_and_regularisation_applied.ipynb">Notebook: ch6_linear_model_selection_and_regularisation_applied.ipynb</a></li>
</ul>
<h3 id="section-9">57. 2018-10-11</h3>
<p>Introduction to Statistical Learning: ch6: <strong>Linear Model Selection and Regularization</strong> Applied:</p>
<ul>
<li>Detailed comparison of model selection techniques in a <strong>simulated</strong> setting. Techniques considered:
<ul>
<li>lasso</li>
<li>ridge regression</li>
<li>backward stepwise</li>
<li>PCR</li>
<li>PLS</li>
</ul></li>
<li><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch6_linear_model_selection_and_regularisation_applied.ipynb">Notebook: ch6_linear_model_selection_and_regularisation_applied.ipynb</a></li>
</ul>
<h3 id="section-10">56. 2018-10-10</h3>
<p>Completed Introduction to Statistical Learning: ch6: <strong>Linear Model Selection and Regularization</strong> Labs:</p>
<ul>
<li>implemented principle components analysis</li>
<li>implemented principle components regression</li>
<li>implemented partial least squares</li>
<li>comparison of principle components regression partial least squares in predicting salaries of baseball players</li>
<li><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch6_linear_model_selection_and_regularisation_labs.ipynb">Notebook: ch6_linear_model_selection_and_regularisation_labs.ipynb</a></li>
</ul>
<h3 id="section-11">55. 2018-10-09</h3>
<p>Working through Introduction to Statistical Learning: ch6: <strong>Linear Model Selection and Regularization</strong> Labs:</p>
<ul>
<li>implemented backward stepwise selection from scratch</li>
<li>compared results from best subset, backward stepwise and forward stepwise selection using cross validation</li>
<li>implemented k-fold cross validation from scratch</li>
<li>completed Lab 1</li>
<li>Lab 2: comparison of ridge regression and the lasso with k-fold cross validation</li>
<li><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch6_linear_model_selection_and_regularisation_labs.ipynb">Notebook: ch6_linear_model_selection_and_regularisation_labs.ipynb</a></li>
</ul>
<h3 id="section-12">54. 2018-10-08</h3>
<p>Read an Introduction to Statistical Learning: ch7: <strong>Moving Beyond Linearity</strong>.</p>
<p>Working through chapter 6 Lab 1: Subset Selection Methods:</p>
<ul>
<li>implemented best subset selection from scratch</li>
<li>implemented forward stepwise selection from scratch</li>
<li><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch6_linear_model_selection_and_regularisation_labs.ipynb">Notebook: ch6_linear_model_selection_and_regularisation_labs.ipynb</a></li>
</ul>
<h3 id="section-13">53. 2018-09-28</h3>
<p>ISL considers ridge regression and the Lasso from both the frequentist and the bayesian perspectives. Although ISL has introduced bayesian statistics and some probability theory, there is clearly much more to this subject.</p>
<p>Researched: what are the best resources for learning probability theory in more depth?</p>
<ul>
<li>Probability Theory: The Logic of Science
<ul>
<li>Lectures: https://www.youtube.com/user/elfpower/videos</li>
<li>Book: https://books.google.co.uk/books/about/Probability_Theory.html?id=tTN4HuUNXjgC&amp;source=kp_book_description&amp;redir_esc=y</li>
</ul></li>
<li>Statistical Rethinking
<ul>
<li>Lectures: https://www.youtube.com/playlist?list=PLDcUM9US4XdM9_N6XUUFrhghGJ4K25bFc</li>
<li>Book: https://www.amazon.com/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/1482253445</li>
<li>Recommendation: https://www.reddit.com/r/statistics/comments/85hxgt/bayesian_statistics_courserecommendation/</li>
</ul></li>
</ul>
<h3 id="section-14">52. 2018-09-27</h3>
<p>An Introduction to Statistical Learning: ch6: <strong>Linear Model Selection and Regularization</strong>.</p>
<p>Completed conceptual exercises:</p>
<ul>
<li>best subset, forward stepwise, backward stepwise selection</li>
<li>the lasso and ridge regression</li>
<li>coefficient penalties</li>
<li><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch6_linear_model_selection_and_regularisation_conceptual.ipynb">Notebook: ch6_linear_model_selection_and_regularisation_conceptual.ipynb</a></li>
</ul>
<h3 id="section-15">51. 2018-09-26</h3>
<p>Revision day. Met with study partner and reviewed our solutions to all exercises from chapters 3-5 of An Introduction to Statistical Learning. Discussed and compared intuitions.</p>
<h3 id="section-16">50. 2018-09-25</h3>
<p>Read ISL ch5: Linear Model Selection and Regularization.</p>
<h3 id="section-17">49. 2018-09-20</h3>
<p>An Introduction to Statistical Learning: ch5: <strong>Resampling Methods</strong>.</p>
<p>Working through applied exercises:<a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch5_resampling_methods_applied.ipynb">My notebook</a></p>
<ul>
<li>implemented LOOCV from scratch</li>
<li>dataset simulation</li>
<li>compared bias variance trade-offs of validation approaches</li>
<li>bootstrap advanced applications</li>
</ul>
<h3 id="section-18">48. 2018-09-19</h3>
<p>An Introduction to Statistical Learning: ch5: <strong>Resampling Methods</strong>.</p>
<p>Working through applied exercises: <a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch5_resampling_methods_applied.ipynb">My notebook</a></p>
<ul>
<li>estimating test error</li>
<li>validation sets</li>
<li>implemented bootstrap from scratch</li>
<li>bootstrap (to estimate standard errors for logistic regression)</li>
</ul>
<h3 id="section-19">47. 2018-09-18</h3>
<p>An Introduction to Statistical Learning: ch5: <strong>Resampling Methods</strong>.</p>
<p>Completed conceptual exercises: <a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch5_resampling_methods_conceptual.ipynb">My notebook</a></p>
<ul>
<li>probability theory review</li>
<li>probability theory and the bootstrap sampling technique</li>
<li>k-fold cross validation</li>
<li>comparison of validation approaches</li>
</ul>
<h3 id="section-20">46. 2018-09-17</h3>
<p>Studied Introduction to Statistical Learning: ch5: <strong>Resampling Methods</strong>.</p>
<h3 id="section-21">45. 2018-09-16</h3>
<p>An Introduction to Statistical Learning: ch4: <strong>Classification</strong>.</p>
<p>Completed applied exercises: <a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch4_classification_applied.ipynb">My notebook</a></p>
<ul>
<li>Logistic regression to classify automobiles by fuel efficiency</li>
<li>Linear discriminant analysis (LDA)</li>
<li>Quadratic discriminant analysis (QDA)</li>
<li>K-nearest neighbour</li>
<li>Comparison of methods</li>
<li>log transforms</li>
</ul>
<h3 id="section-22">44. 2018-09-15</h3>
<p>An Introduction to Statistical Learning: ch4: <strong>Classification</strong>.</p>
<p>Working through applied exercises: <a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch4_classification_applied.ipynb">My notebook</a></p>
<ul>
<li>Logistic regression to predict stock returns</li>
<li>Confusion matrix</li>
<li>Logistic regression statistics, false positive, true negatives etc.</li>
</ul>
<h3 id="section-23">43. 2018-09-14</h3>
<p>An Introduction to Statistical Learning: ch4: <strong>Classification</strong>.</p>
<p>Completed conceptual exercises: <a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch4_classification_conceptual.ipynb">My notebook</a></p>
<ul>
<li>simplification of cost function for logistic regression</li>
<li>bayes classifier</li>
<li>proof that babes classifier is not linear for Quadratic Discriminant Analysis</li>
<li>demonstration of curse of dimensionality for KNN</li>
<li>Comparison of LDA and QDA</li>
<li>Bayes theorem</li>
<li>odds</li>
</ul>
<h3 id="section-24">42. 2018-09-13</h3>
<p>An Introduction to Statistical Learning: ch3: <strong>Linear Regression</strong>.</p>
<p>Completed applied exercises:</p>
<p><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch3_linear_regression_practical.ipynb">My notebook</a></p>
<h3 id="section-25">41. 2018-09-12</h3>
<p>An Introduction to Statistical Learning: ch3: <strong>Linear Regression</strong>.</p>
<p>Working through applied exercises:</p>
<p><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch3_linear_regression_practical.ipynb">My notebook</a></p>
<h3 id="section-26">40. 2018-09-10</h3>
<p>An Introduction to Statistical Learning: ch3: <strong>Linear Regression</strong>.</p>
<p>Working through applied exercises:</p>
<p><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch3_linear_regression_practical.ipynb">My notebook</a></p>
<h3 id="section-27">39. 2018-09-8</h3>
<p>An Introduction to Statistical Learning: ch3: <strong>Linear Regression</strong>.</p>
<p>Working through applied exercises:</p>
<p><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch3_linear_regression_practical.ipynb">My notebook</a></p>
<h3 id="section-28">38. 2018-09-07</h3>
<p>Working on practical exercises from chapter 3 of An Introduction to Statistical Learning. <a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch3_linear_regression_practical.ipynb">My notebook</a>.</p>
<ul>
<li>implemented several key statistics from scratch to bolster my understanding</li>
<li>emulated R's powerful lm().plot() functionality in python, it wasn't available</li>
</ul>
<p>Thoughts:</p>
<p>I'm suprised how much work I had to do to emulate R's lm().plot() functionality. The resultant plots seem really useful, particularly as they provide insight for multivariate models, and anomaly detection in that context. Perhaps worth a blog post to see if useful to others?</p>
<h3 id="section-29">37. 2018-09-06</h3>
<p>Completed exercises on conceptual topics from chapter 3 of An Introduction to Statistical Learning. <a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch3_linear_regression_conceptual.ipynb">My notebook</a>.</p>
<p>Thoughts:</p>
<p>I've noticed that there are different mathematical approaches to linear regression. Hastie and Tibshirani's notation includes a lot of summations, perhaps to avoid linear algebra being pre-requisite, but I find the vectorised forms more intuitive.</p>
<h3 id="section-30">36. 2018-09-05</h3>
<p>Study Linear Regression, chapter 3 of <a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a>.</p>
<ul>
<li>Multiple Linear Regression</li>
<li>Qualitative predictors</li>
<li>Extensions of the linear model</li>
<li>Potential problems</li>
<li>Non-parametric regression: KNN regression</li>
</ul>
<p>Thoughts:</p>
<p>Picked up some ideas on how to improve my House Prices kaggle submission, including:</p>
<ul>
<li>Interaction variables p89</li>
<li>Outlier detection by residual plots p97</li>
</ul>
<p>Had an idea for project using cluster analysis / unsupervised methods for classification of head impact during sport. I've ordered a 9-DOF sensor to hook up to my arduino and gather some data.</p>
<h3 id="section-31">35. 2018-09-03</h3>
<p>Study Linear Regression, chapter 3 of <a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a>.</p>
<ul>
<li>Simple Linear Regression</li>
<li>Multiple Linear Regression</li>
</ul>
<p>Cross referenced with <a href="https://see.stanford.edu/Course/CS229/54">Stanford CS229 lecture 2</a> and Murphy's <em>Machine Learning: A Probabilistic Perspective</em>.</p>
<h3 id="section-32">34. 2018-09-02</h3>
<p>Today I did the applied exercises from chapter 2 of ISL. Here's <a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch2_statistical_learning.ipynb">my notebook</a>. Good exercise in preprocessing and exploration of datasets, sharpening up in Pandas and Matplotlib / Seaborn.</p>
<h3 id="section-33">33. 2018-09-01</h3>
<ul>
<li>Read chapters 1 &amp; 2 of <a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a> (ISL)</li>
<li>Completed <a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch2_statistical_learning.ipynb">conceptual exercises</a> from book</li>
</ul>
<h3 id="section-34">33. 2018-08-31</h3>
<p>Next up a tour of supervised learning techniques. I've decided to take the Stanford course on <a href="https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about">Statistical Learning</a> by Trevor Hastie and Robert Tibshirani. My worfklow for each chapter will be:</p>
<ol style="list-style-type: decimal">
<li>Watch video lectures and complete online quizzes</li>
<li>Review chapter in the accompanying textbook <a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a>, to get more detail</li>
<li>Complete exercises from book</li>
</ol>
<p>Today I completed the video lectures for chapters 1 sand 2.</p>
<h3 id="section-35">32. 2018-08-28</h3>
<p>Kaggle Competition – <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a></p>
<ul>
<li>implemented Lasso regularisation model to gain 400 places in leaderboard</li>
<li>met with study buddy to compare notes and revise</li>
<li>revised regularisation and discussed learnings from Kaggle progress so far</li>
</ul>
<table>
<thead>
<tr class="header">
<th><strong>Leaderboard Results</strong></th>
<th>Description</th>
<th>CV</th>
<th>Public Score</th>
<th>Position</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>v1</td>
<td>Basic LinReg &amp; feature engineering</td>
<td>0.14698</td>
<td>0.13500</td>
<td>1825/4394</td>
</tr>
<tr class="even">
<td>v2</td>
<td>Feature normalisation</td>
<td>0.13360</td>
<td>0.13170</td>
<td>▲ 1610/4394</td>
</tr>
<tr class="odd">
<td>v3</td>
<td>Outlier anomaly detection</td>
<td>0.11436</td>
<td>0.13227</td>
<td>▼</td>
</tr>
<tr class="even">
<td>v6</td>
<td>Lasso</td>
<td>0.12059</td>
<td>0.12361</td>
<td>▲ 1182/4394</td>
</tr>
</tbody>
</table>
<p>Thoughts:</p>
<p>This Kaggle competition has been excellent opportunity to learn about the more practical aspects of machine learning beyond algorithms. Feature engineering and Cross-validation have stuck out as important areas that are not well covered by theoretical study.</p>
<p>I feel like I've taken my Kaggle score as far as I can using a simple linear model and feature engineering. Next I'm going to learn about more advanced models and techniques.</p>
<p>See my <a href="https://www.kaggle.com/alanmartyn/linear-regression">notebook here</a></p>
<h3 id="section-36">31. 2018-08-27</h3>
<p>Kaggle Competition – <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a></p>
<p>Today I looked at improving my results by removing anomalies from my training set. My hypothesis was that removing anomalies would allow my model to better fit the majority of data, at the cost of a worst fit for anomalies. My assumption here was that the error in fitting anomalies would be outweighed by better fit for majority of datapoints.</p>
<ul>
<li>reviewed fundamental statistical measures such as Kurtosis</li>
<li>learned about Dbscan and Isolation Forest techniques for anomaly detection in high-dimensional settings, a useful reference <a href="https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561">here</a>.</li>
<li>reviewed Sklearn documentation and user guides on Isolation Forest</li>
<li>Implemented Isolation Forest</li>
<li>Observed over-fitting improvement but no measurable improvement on test data – highlighting the importance of a good CV :(</li>
</ul>
<table>
<thead>
<tr class="header">
<th><strong>Leaderboard Results</strong></th>
<th>Description</th>
<th>CV</th>
<th>Public Score</th>
<th>Position</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>v1</td>
<td>Basic LinReg &amp; feature engineering</td>
<td>0.14698</td>
<td>0.13500</td>
<td>1825/4394</td>
</tr>
<tr class="even">
<td>v2</td>
<td>Feature normalisation</td>
<td>0.13360</td>
<td>0.13170</td>
<td>▲ 1610/4394</td>
</tr>
<tr class="odd">
<td>v3</td>
<td>Outlier anomaly detection</td>
<td>0.11436</td>
<td>0.13227</td>
<td>▼</td>
</tr>
</tbody>
</table>
<p>See my <a href="https://www.kaggle.com/alanmartyn/linear-regression">notebook here</a></p>
<h3 id="section-37">30. 2018-08-26</h3>
<p>Kaggle Competition – <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a></p>
<p>Today I focused on minimising exponential characteristics in to make relationships in the dataset more linear. This allows my simple linear model to fit with reduced error. This jumped me 200 places in the leaderboard.</p>
<table>
<thead>
<tr class="header">
<th><strong>Leaderboard Results</strong></th>
<th>Description</th>
<th>CV</th>
<th>Public Score</th>
<th>Position</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>v1</td>
<td>Basic LinReg &amp; feature engineering</td>
<td>0.14698</td>
<td>0.13500</td>
<td>1825/4394</td>
</tr>
<tr class="even">
<td>v2</td>
<td>Feature normalisation</td>
<td>0.13360</td>
<td>0.13170</td>
<td>▲ 1610/4394</td>
</tr>
</tbody>
</table>
<p>See my <a href="https://www.kaggle.com/alanmartyn/linear-regression">notebook here</a></p>
<h3 id="section-38">29. 2018-08-25</h3>
<p>Kaggle Competition – <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a></p>
<table>
<thead>
<tr class="header">
<th><strong>Leaderboard Results</strong></th>
<th>Description</th>
<th>CV</th>
<th>Public Score</th>
<th>Position</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>v1</td>
<td>Basic LinReg &amp; feature engineering</td>
<td>0.14698</td>
<td>0.13500</td>
<td>1825/4394</td>
</tr>
</tbody>
</table>
<ul>
<li>My first submission to leaderboard using the most basic Multivariate Linear Regression model. I'm quite pleased with the result given that this is using a basic model. My efforts in feature engineering seem to have paid off.</li>
<li>Read <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html">In Depth: Linear Regression</a> from VanderPlas' book</li>
</ul>
<p>See my <a href="https://www.kaggle.com/alanmartyn/linear-regression">notebook here</a></p>
<h3 id="section-39">28. 2018-08-24</h3>
<p>Kaggle Competition – <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a></p>
<ul>
<li>Reviewed Andrew NG's advice on model validation</li>
<li>Read <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html">Hyperparameters and Model Validation</a> chapter from Jake VanderPlas</li>
<li>Decided that a solid cross-validation (CV) is crucial</li>
<li>bolstered understanding by implementing my own hold-out and cross-validation functions</li>
<li>reviewed cross validation features in SkLearn</li>
<li>tested above CV's and chose method</li>
</ul>
<p>Thoughts:</p>
<p>Cross-validation like feature engineering is another topic that seems critical to practical application of ML, yet is not widely covered in the literature I've seen so far. The current 1 Kaggler, Shubin Dai (bestfitting), puts a lot of weight on designing good CVs as part of his approach described in this <a href="http://blog.kaggle.com/2018/05/07/profiling-top-kagglers-bestfitting-currently-1-in-the-world/">interview</a></p>
<p>During my own experimentation I observed surprisingly high variation between different approaches.</p>
<p>See my <a href="https://www.kaggle.com/alanmartyn/linear-regression">notebook here</a></p>
<h3 id="section-40">27. 2018-08-23</h3>
<p>Today I focused on visualisation and feature engineering of data for the Kaggle competition – <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a></p>
<ul>
<li>indexed data by statistical data types using Pandas</li>
<li>applied numeric mappings for ordinal data</li>
<li>OneHot treatment of categorical data</li>
<li>thought carefully about how to handle missing data</li>
<li>exploratory analysis with Matplotlib and Seaborn</li>
</ul>
<p>Thoughts:</p>
<p>Spending time on visualisation helped me to think clearly about the data. The ability to knock out a quick visualisation to check my intuition seems invaluable.</p>
<p>Feature engineering can be quite laborious, but seems like one of the most critical parts of the process because a mis-step here could result in information loss or distortion. For example, filling missing values in YearBuilt with zeros makes no sense, whilst filling WoodenPatioArea with mean values might inadvertently mis-label properties that don't have such a feature.</p>
<p>Content on feature engineering in books and online seems light. Wondering if there's opportunities for improved tooling here too?</p>
<p>This <a href="https://vimeo.com/274274744">talk by Andrej Karpathy</a> from Tesla, gives an interesting insight on the importance of feature engineering at Tesla.</p>
<p>See my <a href="https://www.kaggle.com/alanmartyn/linear-regression">notebook here</a></p>
<h3 id="section-41">26. 2018-08-22</h3>
<p>Today I started the Kaggle competition – <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a></p>
<p>To get up and running I:</p>
<ul>
<li>read up on Kaggle competitions</li>
<li>looked through the data descriptions in detail</li>
<li>setup my Jupyter Notebook/Kernel</li>
<li>reviewed <a href="https://en.wikipedia.org/wiki/Statistical_data_type">Statistical Data</a> types</li>
<li>reviewed visualisation with Matplotlib</li>
<li>reviewed data manipulation with Pandas</li>
<li>Read chapter on <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html">Feature Engineering</a> from Python Data Science Handbook by Jake VanderPlas</li>
</ul>
<p>Thoughts:</p>
<p>The Kaggle dataset is not as 'clean' as I imagined. There is plenty of inconsistency in the formatting and labelling that will need to tackled before it can be passed to a model. I'm going to segment real, count, ordinal and categorical data in case I need to process them separately.</p>
<h3 id="section-42">25. 2018-08-21</h3>
<p>Meet with study buddy to revise entire <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a> course. Focus on areas we found most difficult. We cover all items under 'Revisit' in this data log. It was really useful to talk through and test each others understanding, felt like I came away with much improved intution in key areas such as numpy broadcasting, inner products and Taylor Series Approximation.</p>
<p>Our search for suitable London housing datasets was fruitless. The Land Registry datasets include sale prices but not much else, we conclude that best approach would be to scrape archived classified listings.</p>
<p>We decide to start by tackling the <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a> Kaggle competition.</p>
<h3 id="section-43">24. 2018-08-17</h3>
<p>Having completed the <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a> course, I met up with my study buddy to do plan next steps and revise the completed course.</p>
<p>We decide to do some applied work next week to predict house prices. Ideally we'd like to predict London house prices, so we decide to look for datasets.</p>
<h3 id="section-44">23. 2018-08-16</h3>
<p><strong>COMPLETE</strong> <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: <strong>Principal Component Analysis</strong> week 4 including:</p>
<ul>
<li>Programming assignment:
<ul>
<li>principle component analysis of MNIST dataset</li>
<li>benchmarking performance</li>
<li>optimisation of PCA for highly dimensional datasets</li>
</ul></li>
</ul>
<h3 id="section-45">22. 2018-08-15</h3>
<p><a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: <strong>Principal Component Analysis</strong> week 4 including:</p>
<ul>
<li>Vector spaces</li>
<li>Orthogonal compliments</li>
<li>Principle Compenent Analysis (PCA) objective</li>
<li>Multivariate chain rule</li>
<li>PCA mathematical proof</li>
<li>Lagrange multipliers revision</li>
</ul>
<p>Revisit:</p>
<ul>
<li>Chain rule practice q.2</li>
</ul>
<h3 id="section-46">21. 2018-08-14</h3>
<p><a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: <strong>Principal Component Analysis</strong> week 3 including:</p>
<ul>
<li>Projection onto 1D subspaces (general case using inner products)</li>
<li>Projection onto higher dimensional subspaces</li>
<li>Orthogonal projections</li>
<li>Programming assignment:
<ul>
<li>eigenfaces</li>
<li>auto-encoder for Olivetti faces dataset</li>
<li>linear regression for boston house prices</li>
</ul></li>
</ul>
<h3 id="section-47">20. 2018-08-13</h3>
<p><a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: <strong>Principal Component Analysis</strong> week 2 including:</p>
<ul>
<li>Basis vectors revision</li>
<li>Inner product angles and orthogonality</li>
<li>exam: angles between vectors usin non-standard inner products</li>
<li>programming assignment: inner products, angels and k-nearest-neighbour</li>
</ul>
<p>Revisit:</p>
<ul>
<li>Programming Assignment: Inner products and angles def pairwise_distance_matrix(X, Y)</li>
<li>What areas of this programming assignement required inner product generalisations?</li>
</ul>
<h3 id="section-48">19. 2018-08-12</h3>
<ul>
<li>I took the first 8 lectures in Pavel Grinfeld's <a href="https://www.youtube.com/playlist?list=PLlXfTHzgMRULZfrNCrrJ7xDcTjGr633mm&amp;disable_polymer=true">inner product course</a></li>
<li>Mathematics for Machine Learning](https://www.coursera.org/specializations/mathematics-machine-learning): <strong>Principal Component Analysis</strong> week 2 including:
<ul>
<li>properties of Inner Products</li>
<li>Inner product generalisations</li>
<li>2 exams</li>
</ul></li>
</ul>
<p>Revisit:</p>
<ul>
<li>Inner product: distances between vectors: I can't replicate Deisenroth's result <span class="citation">@3:00</span>, 12^0.5</li>
</ul>
<h3 id="section-49">18. 2018-08-11</h3>
<p>Studied course <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: <strong>Principal Component Analysis</strong> week 1 &amp; 2 including:</p>
<ul>
<li>programming assignment to calculate mean and covariance of <a href="http://scikit-learn.org/stable/datasets/olivetti_faces.html">The Olivetti Faces</a> dataset
<ul>
<li>implemented naive iterative mean and covariance functions in Python</li>
<li>benchmarked against equivalent functions in Numpy</li>
</ul></li>
<li><p>data visualisation: worked through Matplotlib <a href="https://matplotlib.org/tutorials/index.html">tutorials</a> and a <a href="http://pbpython.com/effective-matplotlib.html">helpful introduction to matplotlib</a> from Chris Moffit</p></li>
<li>linear algebra: Dot Product review</li>
<li>linear algebra: Inner Products. Quite an abstract concept not well motivated by M4ML course, I found these lectures particularly useful.
<ul>
<li><a href="https://www.youtube.com/watch?v=Ww_aQqWZhz8">Why inner products?</a></li>
<li><a href="https://www.youtube.com/watch?v=8M6eo3j7jO4">Identifying inner products</a></li>
</ul></li>
</ul>
<h3 id="section-50">17. 2018-08-10</h3>
<p>Started course <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: <strong>Principal Component Analysis</strong> week 1 including:</p>
<ul>
<li>mean of high dimensional datasets</li>
<li>variance of 1D datasets</li>
<li>variance of high dimensional datasets</li>
<li>covariance</li>
</ul>
<p>I read the Principle Compenent Analysis section in Hastie &amp; Tibshirani's Introduction to Statistical Learning textbook.</p>
<h3 id="section-51">16. 2018-08-08</h3>
<p><strong>Completed course:</strong> <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: <strong>Multivariate Calculus</strong></p>
<p>Studied week 6 including:</p>
<ul>
<li>Linear regression</li>
<li>General non-linear least squares</li>
<li>Least squares regression analysis in practice</li>
<li>Programming assignment: fitting the distribution of height data</li>
</ul>
<p>I listened to <a href="https://robohub.org/talking-machines-history-of-machine-learning-w-geoffrey-hinton-yoshua-bengio-yann-lecun/">The History of Machine Learning from the Inside Out</a> podcast where Geoffrey Hinton, Yoshua Bengio and Yann LeCun discuss motivations for deep-learning before state of the art results. Useful terminology, and interesting to hear their emphsasis on unsupervised learning as the area for future advances.</p>
<h3 id="section-52">15. 2018-08-07</h3>
<p>Studied <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: <strong>Multivariate Calculus</strong> week 5 including:</p>
<ul>
<li>Gradient descent strategies</li>
<li>Newton-Raphson revision</li>
<li>Constrained optimisation</li>
<li>Lagrange multipliers</li>
</ul>
<p>Great fun experimenting with gradient descent methods in the exercises. I submitted a proposal to the course forums for a gradient descent method that seems more robust to steep sided gulleys and shallow gradients. Coursework is starting to give me a good orientation for Jupyter notebooks, scipy, and numpy.</p>
<p>I wanted more detail on constrained optimisation so I also took the Khan Academy course, <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/constrained-optimization-introduction">Lagrange multipliers and constrained optimization</a>, which taught me about The Lagrangian and also steps through the proofs.</p>
<p>I finished the first chapter of Kevin Murphy's Machine Learning book.</p>
<h3 id="section-53">14. 2018-08-06</h3>
<p>Studied <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: <strong>Multivariate Calculus</strong> week 4 + 5 including:</p>
<ul>
<li>Linearisation revision</li>
<li>Multivariate Taylor series</li>
<li>Taylor series exam</li>
<li>Newton-Raphson iterative optimisation method</li>
</ul>
<p>Revisit:</p>
<ul>
<li>week 4 / 2D Taylor series / q4 improve notes on linearisation, and ask forum question</li>
<li>week 4 / Taylor Series Assessment / q4, why is f(x) = (x/2)^2​ sin(2x)/2 odd? Why are functions that are same when rotated 180deg about the origin odd?</li>
</ul>
<h3 id="section-54">13. 2018-08-05</h3>
<p>Studied <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: <strong>Multivariate Calculus</strong> week 4 including:</p>
<ul>
<li>Taylor Series</li>
<li>Power Series</li>
<li>Maclaurin Series</li>
<li>Linearisation</li>
</ul>
<p>exercises and exams, at this stage of the course, continue to extend practical abilities in finding derivatives of complex functions. I found this demonstration of the <a href="https://www.youtube.com/watch?v=h_-W4nqy-yY">Binomial Theorem</a> useful because it provides a neat way to layout working on the page.</p>
<p>Revisit:</p>
<ul>
<li>week 4 / Taylor series - Special cases / Q4 – why is f(x) = 1/(1+x)^2 discontinuous in the complex plane?</li>
</ul>
<h3 id="section-55">12. 2018-08-04</h3>
<p>Studied <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: <strong>Multivariate Calculus</strong> week 4 including: - Building approximate functions - Power series</p>
<p>Implemented backpropagation in Python with Numpy. That's the 3rd time I've implemented backprop now, having a detailed knowledge of the chain rule was really useful. This time around I feel like I've grokked it.</p>
<h3 id="section-56">11. 2018-08-03</h3>
<p>Studied <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: <strong>Multivariate Calculus</strong> week 3 including:</p>
<ul>
<li>Multivariate chain rule</li>
<li>Simple neural networks</li>
<li>Feedforward</li>
<li>Backpropagation</li>
</ul>
<p>Great to revisit the basic neural nets example with the linear algebra and calculus principles built up so far. Satisfying to combine linear algebra with the chain rule.</p>
<p>I read the article <a href="http://www.fast.ai/2018/07/23/auto-ml-3/">Google's AutoML: Cutting Through the Hype</a> which offers an interesting perspective on the commercialisation of ML tech, some potential pitfalls for big companies in this area.</p>
<h3 id="section-57">10. 2018-08-02</h3>
<p>Studied <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: <strong>Multivariate Calculus</strong> week 2 including:</p>
<ul>
<li>Partial differentiation</li>
<li>The Jacobian</li>
<li>The Hessian</li>
</ul>
<p>Pretty cool to start combining linear algebra with calculus here. Looking at applications of the Jacobian and the Hessian to gradient descent problems – like the saddle problem – is a great motivator.</p>
<p>I read the paper <a href="https://www.nature.com/articles/s41746-018-0029-1">Scalable and accurate deep learning with electronic health records</a> that reports on Google's latest work applying ML in healthcare. Looks like they’ve improved upon previous predictions by about 10% accuracy, but the thing that is most interesting is how they've achieved this without need for laborious cleaning and pre-processing of data. They have developed a pipeline that takes raw unstructured data like handwritten notes as input, and outputs some more structure FHIR compliant output.</p>
<p>There's also a ux that allows doctors to inspect what parts of a given health record were most significant in generating the prediction, a step away from black-box characteristics. Finally, three different models are used (RNN, LSTM, Gradient Boosting), they are able to select the most appropriate model for a particular context using a method called Ensembling.</p>
<h3 id="section-58">9. 2018-08-01</h3>
<p>I met up with my study buddy to discuss progress. We decided to focus primarily on theoretical track for next 2 weeks to get foundational maths in place. Up until now we have found it difficult to split away from that track, and the content feels critical to more applied study.</p>
<p>I started the <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: <strong>Multivariate Calculus</strong> 6 week course.</p>
<p>Revised differentiation basics including Product Rule, Chain Rule, and practice exercises.</p>
<p>I got some advice from friends on best practices for self-directed study. I'm now separating my notes into two notebooks: 1) clean revision notes 2) messy exercise notes. Each morning I now reinforce my understanding by recalling concepts covered the previous day, I then review my notes as reinforcement. I'll also start revisiting the harder exercise questions as further reinforcement.</p>
<h3 id="section-59">8. 2018-07-31</h3>
<p>I completed the <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: Linear algebra course, completing final exam.</p>
<p>I went back an reviewed some of the principles covered in more detail, by reviewing topics on Khan Academy including: <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/matrices-elimination/v/matrices-reduced-row-echelon-form-1">reduced row echelon form</a>, rank of matrix, calculating determinants in the general case and so on.</p>
<p><strong>Thoughts:</strong></p>
<p>There's definitely more to learn about Linear Algebra than could be covered in the Imperial course, but it has given me a good sense of orientation and a desire to continue study in this area. My additional studies on Khan Academy have closed the loop on some areas I was intrigued by such as rref, and rank.</p>
<h3 id="section-60">7. 2018-07-30</h3>
<p><a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: Linear algebra course, covered: - study of eigenvectors - implementation of Google-esque PageRank algorithm which finds largest eigenvector with power method</p>
<h3 id="section-61">6. 2018-07-29</h3>
<ul>
<li><a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: Linear algebra. Covered:
<ul>
<li>Einstein summation convention and the symmetry of the dot product</li>
<li>Exam: Matrix multiplication</li>
<li>Matrices transform in a new basis vector set</li>
<li>Exam: Mappings to spaces with different numbers of dimensions</li>
<li>Orthogonal matrices</li>
<li>Gram-Schmidt process</li>
<li>Coursework: implemented Gram-Schmidt algo.</li>
<li>Reflecting in a plane</li>
<li>Coursework: Reflecting in a plane with Numpy</li>
<li>Week 4 complete</li>
<li>Eigenvalues and eigenvectors concepts</li>
<li>Exam: selecting eigenvectors by inspection</li>
<li>Special eigen-cases</li>
<li>Calculating eigenvectors</li>
<li>Exam: Characteristic polynomials, eigenvalues and eigenvectors</li>
</ul></li>
</ul>
<p><strong>Thoughts:</strong></p>
<p>A heads down day studying linear algebra. The exams provide a lot of exercise which really drive things home. Would be good to find ways to keep this knowledge fresh, perhaps longer term exercises?</p>
<p><strong>Thoughts:</strong></p>
<p>Great to work on some hands-on examples manipulating information with matrices. Techniques feel generally applicable. Insights on how to measure the information available in a set of vectors, e.g. are they linearly dependant? will be valuable in assessing/designing datasets.</p>
<h3 id="section-62">5. 2018-07-28</h3>
<p><strong>Today's progress:</strong></p>
<ul>
<li>Met up with a friend who as good applied data science experience. He has built a data science team and mentored students. I learned about the skill level of people working in the field, their day-to-day work and gained insight on best practices and pitfalls.</li>
<li><a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: Linear algebra. Covered:
<ul>
<li>Solving linear equations using the inverse matrix I</li>
<li>Determinants and inverses</li>
<li>Matrix identification programming assignment</li>
<li>finished week 3</li>
</ul></li>
</ul>
<p><strong>Thoughts:</strong></p>
<p>Data Science practitioners often bring with them a specific technique, perhaps based on past specialisation. It seems as though some practitioners might be inflexible in the range of techniques they will bring to bear on a problem. I wonder if it is realistic to gain a deep – or at least practical – understanding in a broad range of techniques?</p>
<p>The ability to build a solid data processing pipeline could be an invaluable practical skill – Kafka might be worth investigating here. Maybe this is something I can collaborate with my study partner on.</p>
<p>Possible avenues for applied projects include: - offering my services to a uni research dept. - kaggle competitions - self defined projects (we worked on some ideas here)</p>
<h3 id="section-63">5. 2018-07-27</h3>
<p><strong>Today's progress:</strong></p>
<ul>
<li><a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: Linear algebra. Covered:
<ul>
<li>transforms matrices</li>
<li>matrix composition and combination</li>
<li>inverse matrices</li>
<li>Gaussian elimination</li>
<li>finding inverse matrix with Gaussian elimination</li>
</ul></li>
</ul>
<p><strong>Thoughts:</strong></p>
<p>Continued with David Dye's course. Had to fix my front door lock so that was a distraction. The exercises in week 3 require quite laborious calculations with lots of basic arithmetic. It's important to be very careful not to cause cascading errors by dropping a minus sign or other basic arithmetic errors. I mad a few. Now I understand how to derive the inverse of a matrix and , and its given me a good intuition for what goes on when you call inv(A)!</p>
<h3 id="section-64">4. 2018-07-26</h3>
<p><strong>Today's progress:</strong></p>
<ul>
<li>Foundational: Studied <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>: Linear algebra. Covered:
<ul>
<li>changing basis for vector operations</li>
<li>basis, vector space, and linear independence</li>
<li>applications of changing basis</li>
<li>passed 3 exams on the above</li>
<li>(week 2 complete)</li>
</ul></li>
<li>Interview training, reviewed:
<ul>
<li>big O notation</li>
<li>data structures: strings, arrays, dynamic lists, linked lists</li>
<li>logarithms</li>
<li>(interviewcake chap. 0 complete)</li>
</ul></li>
<li>Started reading Murphy's textbook.</li>
</ul>
<p><strong>Thoughts:</strong></p>
<p>It took me much longer to complete MfML week 2 than expected, time was spent working through the exam questions - most of time spent filling out 17 pages in my notebook with workings! I tried to make sure I could derive formula rather than rote memorisation. Cross checking with my study partner revealed a formula I'd derived incorrectly so that was definitely useful, was also good to practice talking the language. David Dye was light on practical method for testing linear dependence – Sal Khan's <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/linear-independence/v/more-on-linear-independence">More on Linear Independence</a> gave the detail I needed.</p>
<p>Interview training just reading so far. Looking forwards to trying out some exercises.</p>
<p>Murphy's is a very exciting book. Is accessible, and covers some prerequisites like probability. Might be a good companion text for MMfL PCA course. Murphy considers that broadly there are 3 domains of ML; supervised learning, unsupervised learning and reinforcement learning – the latter not being covered in this book. He recognises the similarities between statistics and ML, and suggests that although topics are broadly similar the emphasis is different.</p>
<p>Tomorrow I'd like to get at least week 3 of MMfL done and chap 1 of interview training.</p>
<p>Finally, my grandmother's 2-year old prediction that ethics will increasingly be a defining consideration in ML applications seems to be bolstered <a href="https://www.bbc.co.uk/news/technology-44977366">today</a> as I read about biases hampering deployment of facial recognition systems. Throws up interesting questions like: How to measure bias in a dataset? How to measure bias in a model?</p>
<h3 id="section-65">3. 2018-07-25</h3>
<p><strong>Today's progress:</strong></p>
<ul>
<li>implemented backpropagation algorithm with regularisation from scratch in Matlab to complete MLNG coursework (week 5 complete)</li>
<li>began study of <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a>. covered:
<ul>
<li>The relationship between machine learning, linear algebra, and vectors and matrices</li>
<li>Vectors: basic operations, modulus &amp; inner product, cosine &amp; dot product, projection</li>
<li>almost finished week2</li>
</ul></li>
</ul>
<p><strong>Thoughts:</strong></p>
<p>The mathematics for machine learning course has been useful, so far I've learned a range of vector operations. The course works up from first principles which means you can get a long way by just recalling the fundamentals and working up from their. Much better suited for me than remembering a load of formulae without context. I also think it is a very good compliment for Andrew Ng's more applied course, certainly goes deeper on the math. I've dug out one of my undergrad textbooks which covers same material, K.A. Stroud's Engineering Mathematics – useful reference for exercise but I'm finding David Dyes lecture's move me along much faster.</p>
<p>Glad to finish the backprop coursework. It was amazing to see hidden layers visualised for MNIST, I've seen those kinds of visualisations before but now I feel like I have a strong intuition on how to interpret them.</p>
<p>Kevin Murphy's seminal book just arrived. Linear regression doesn't appear until page 219, and deep learning not until 999, so I'd better get to it.</p>
<h3 id="section-66">2. 2018-07-24</h3>
<p><strong>Today's progress:</strong></p>
<ul>
<li>revisited Andrew Ng's ML week 5 lectures on neural nets, feedworward, and backpropagation.</li>
<li>Implemented feedfoward for MNIST example in Matlab</li>
<li>reviewed and responded to great feedback on curriculum and motivations from professionals in the field</li>
<li>listened to Talking Machines podcast episode 1</li>
<li>had cursory skim through Google's new <a href="https://developers.google.com/machine-learning/guides/">Machine Learning guides</a>, these look useful, might fold into curriculum</li>
<li>started building a list of project ideas, i need to publish these</li>
<li>added <a href="https://github.com/a-martyn/ml-sabbatical/tree/master/coursework-ng">my coursework</a> from Andrew Ng's Machine Learning course to this repo</li>
</ul>
<p><strong>Thoughts:</strong></p>
<p>Very useful feedback from mentors. Key points included:</p>
<ul>
<li>find a problem that I'm compelled to solve, then use that as project and curriculum benchmark</li>
<li>find ways to teach what I've learned to others at a similar level</li>
<li>mentors encouraged me to clarify my thinking on where I want to be in 3 months</li>
</ul>
<p>This will all be really useful for informing curriculum.</p>
<p>Great to get back up and running with Ng's course. Ng recommends implementing feedforward with a <code>for</code> loop, but as I was implementing it I noticed that the loop could be removed by applying linear algebra. I decided to stick with Ng's recommendation, I think he recommends this approach because it allows you to break down the process into very small incremental steps. This proved useful during debugging, and I think it gave me a better intuition than if I'd used abstractions.</p>
<p>When implementing feedforward I was getting an unexpected value. I was able to debug it by isolating one training example as a test case, and then using intuition to notice that the cost should not be a negative value. I didn't get as far on the coursework as I'd hoped, but feel like I'll be faster today.</p>
<h3 id="section-67">1. 2018-07-23</h3>
<p><strong>Today's progress:</strong></p>
<ul>
<li>[x] define <a href="https://github.com/a-martyn/ml-sabbatical/tree/master/docs/005-curriculum.md">curriculum</a></li>
<li>[x] setup this learning log</li>
<li>[x] share <a href="https://github.com/a-martyn/ml-sabbatical/tree/master/docs/005-curriculum.md">curriculum</a> and <a href="(https://github.com/a-martyn/ml-sabbatical/tree/master/docs/002-curriculum.md)">motivations</a> with friends for feedback</li>
<li>[x] ordered some textbooks</li>
</ul>
<p><strong>Thoughts:</strong></p>
<p>It's day 1. Well not quite, I've spent the last few days researching the available learning resources including courses and textbooks. There's a lot out there, I started to feel a bit lost at sea, so I decided to use the UCL MSC in machine learning as a basis, and focused on material that covered prerequisite foundational material as well the core unsupervised learning module, here's my <a href="https://github.com/a-martyn/ml-sabbatical/tree/master/docs/004-books-and-courses.md">notes</a>.</p>
<p>Today I met up with my study partner to compare notes and pin down a <a href="https://github.com/a-martyn/ml-sabbatical/tree/master/docs/005-curriculum.md">curriculum</a>. It is a big relief to have this down on paper. Looking forwards to getting into it tomorrow.</p>
</div>
</body>
</html>
