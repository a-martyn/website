<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-137570937-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-137570937-1');
  </script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Alan Martyn</title>
  
  <!-- Bootstrap core CSS -->
  <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom CSS -->
  <link href="css/markdown.css" rel="stylesheet">

</head>
<body>
  <!-- Navigation -->
<nav class="navbar navbar-expand-lg navbar-dark static-top" style="background-color: #619668;">
  <div class="container">
    <a class="navbar-brand" href="https://www.alanmartyn.com">Alan Martyn</a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarResponsive">
      <ul class="navbar-nav ml-auto">
        
        <li class="nav-item">
          <a class="nav-link" href="https://www.alanmartyn.com/content/cv.html">CV</a>
        </li>
        
      </ul>
    </div>
  </div>
</nav>

  <div class="content">
  <h2 id="machine-learning-curriculum">Machine Learning: Curriculum</h2>
<p>This is the curriculum I’ve followed with the goal of understanding machine learning from first principles whilst also building practical experience applicable to real-world problems. I’d be thrilled if this record of my path is useful to anyone else who is approaching machine learning from a background in software development or otherwise. If you’d like to discuss please dm me on twitter or email.</p>
<h3 id="starting-point">Starting point</h3>
<ul>
<li>Undergraduate degree in Audio Engineering covering audio signal processing, compression techniques, auditory perception, mathematics and electrical engineering</li>
<li>6 years experience working in software development and prototyping for iOS and web</li>
<li>2-3 years experience in python development for backend web services and some data analysis</li>
<li>lots of messing around with Arduino microprocessors</li>
<li>A-level and undergraduate mathematics in need of refresh</li>
</ul>
<h3 id="machine-learning-by-andrew-ng">1. <a href="https://www.coursera.org/learn/machine-learning">Machine Learning</a> by Andrew Ng</h3>
<p><em>(Stanford / Coursera)</em></p>
<p>A broad high-level introduction to machine learning techniques. A good motivational course but I stopped after week 5 because I felt it was too high-level, instead I wanted to approach the subject bottom-up from mathematical principles.</p>
<p>My code solutions are on <a href="https://github.com/a-martyn/ml-sabbatical/tree/master/coursework-ng">Github here</a>.</p>
<ul>
<li>Linear Regression with One Variable</li>
<li>Linear Algebra Review</li>
<li>Linear Regression with Multiple Variables</li>
<li>Octave/Matlab Tutorial</li>
<li>Logistic Regression</li>
<li>Regularisation</li>
<li>Neural Networks</li>
</ul>
<h3 id="mathematics-for-machine-learning">2. <a href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a></h3>
<p><em>(Imperial College / Coursera)</em></p>
<p>A good introduction to some of the prerequisite mathematics for machine learning covering Linear Algebra, Multivariate Calculus and statistical methods. I feel that probability theory is missing.</p>
<p>See my code on <a href="https://github.com/a-martyn/ml-sabbatical/tree/master/coursework-maths4ml">Github here</a>.</p>
<h5 id="course-1-linear-algebra">Course 1: Linear Algebra</h5>
<p><em>Lecturer: David Dye</em></p>
<ul>
<li>The relationship between machine learning, linear algebra, and vectors and matrices</li>
<li>Vectors</li>
<li>Modulus &amp; inner product</li>
<li>Dot product</li>
<li>Projection</li>
<li>Basis, vector space, and linear independence</li>
<li>Changing basis</li>
<li>Matrices in linear algebra: operating on vectors</li>
<li>Matrix Inverses, Gaussian elimination</li>
<li>Matrix Determinants and Inverses</li>
<li>Einstein summation</li>
<li>Non-square matrix multiplication</li>
<li>Matrix transforms, mapping and changing basis</li>
<li>Orthogonal matrices</li>
<li>The Gram-Schmidt process</li>
<li>Eigenvalues and eigenvectors</li>
<li>Eigenvector application: implementation of Page Rank algorithm</li>
</ul>
<h5 id="course-2-multivariate-calculus">Course 2: Multivariate Calculus</h5>
<p><em>Lecturer: Samuel J. Cooper</em></p>
<ul>
<li>Gradients and derivatives</li>
<li>Multivariate Product Rule &amp; Chain Rule</li>
<li>Multivariate calculus</li>
<li>Partial differentiation</li>
<li>The Jacobian</li>
<li>The Hessian</li>
<li>Multivariate chain rule</li>
<li>Implementation of back propagation and neural net</li>
<li>Multivariate Taylor Series approximations</li>
<li>Optimisation</li>
<li>Newton-Raphson method</li>
<li>Implementation and comparison of gradient descent methods</li>
<li>Constrained optimisation &amp; Lagrange multipliers</li>
<li>Linear regression</li>
<li>General non-linear least squares</li>
</ul>
<h5 id="course-3-principle-components-analysis">Course 3: Principle Components Analysis</h5>
<p><em>Lecturer: Marc P. Deisenroth</em></p>
<ul>
<li>Mean of datasets</li>
<li>Variances and covariances</li>
<li>Linear transformation of datasets</li>
<li>Dot product</li>
<li>Inner products</li>
<li>Projections</li>
<li>Principle Component Analysis derivation</li>
<li>Principle Component Analysis implementation</li>
</ul>
<h5 id="supporting-study">Supporting study:</h5>
<ul>
<li>Pavel Grinfeld’s <a href="https://www.youtube.com/playlist?list=PLlXfTHzgMRULZfrNCrrJ7xDcTjGr633mm&amp;disable_polymer=true">inner product course</a></li>
<li><a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/constrained-optimization-introduction">Lagrange multipliers and constrained optimisation</a> – Khan Academy</li>
<li><a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/matrices-elimination/v/matrices-reduced-row-echelon-form-1">Reduced row echelon form</a> – Khan Academy</li>
<li><a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/linear-independence/v/more-on-linear-independence">More on Linear Independence</a> - Khan Academy</li>
<li>Matplotlib <a href="https://matplotlib.org/tutorials/index.html">tutorials</a> and a <a href="http://pbpython.com/effective-matplotlib.html">helpful introduction to matplotlib</a> from Chris Moffit</li>
</ul>
<h3 id="house-prices-kaggle">3. <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices</a>: Kaggle</h3>
<p>Although all courses included in the curriculum so far include practical exercises they are quite heavily guided so I wanted a more free-form applied setting. For this I turned to Kaggle.</p>
<p>See my <a href="https://www.kaggle.com/alanmartyn/linear-regression">notebook here</a>. Disclaimer – a lot of naive work here which lead me to pick up <em>An Introduction to Statistical Learning</em> next.</p>
<h5 id="supporting-study-1">Supporting study:</h5>
<ul>
<li><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html">Python Data ScienceHandbook</a> by Jake VanderPlas</li>
</ul>
<h3 id="an-introduction-to-statistical-learning">4. <a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a></h3>
<p><em>Trevor Hastie and Robert Tibshirani (Textbook)</em></p>
<IMG src='http://www-bcf.usc.edu/%7Egareth/ISL/ISL%20Cover%202.jpg' height=20% width=20%>
<P>
<p>To build a conceptual and applied understanding of supervised learning techniques I chose to work through An Introduction to Statistical Learning cover-to-cover, completing the exercise at the end of each chapter. Conceptual exercises cover the mathematics, whilst applied exercises focus on prediction and inference tasks using various datasets.</p>
<p>The textbook targets the R programming language, but I decided to implement my solutions in Python. Where Python lacked R’s functionality I implemented from scratch – I found this to be a useful means to build intution.</p>
<p>Links to my notebooks below:</p>
<p><a href="http://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch2_statistical_learning_conceptual.ipynb">Chapter 2 - Statistical Learning: Conceptual</a><br />
<a href="http://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch2_statistical_learning_applied.ipynb">Chapter 2 - Statistical Learning: Applied</a></p>
<p><a href="http://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch3_linear_regression_conceptual.ipynb">Chapter 3 - Linear Regression: Conceptual</a><br />
<a href="http://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch3_linear_regression_applied.ipynb">Chapter 3 - Linear Regression: Applied</a></p>
<p><a href="http://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch4_classification_conceptual.ipynb">Chapter 4 - Classification: Conceptual</a><br />
<a href="http://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch4_classification_applied.ipynb">Chapter 4 - Classification: Applied</a></p>
<p><a href="http://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch5_resampling_methods_conceptual.ipynb">Chapter 5 - Resampling Methods: Conceptual</a><br />
<a href="http://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch5_resampling_methods_applied.ipynb">Chapter 5 - Resampling Methods: Applied</a></p>
<p><a href="http://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch6_linear_model_selection_and_regularisation_labs.ipynb">Chapter 6 - Linear Model Selection and Regularization: Labs</a><br />
<a href="http://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch6_linear_model_selection_and_regularisation_conceptual.ipynb">Chapter 6 - Linear Model Selection and Regularization: Conceptual</a><br />
<a href="http://nbviewer.jupyter.org/github.com/a-martyn/ISL-python/blob/master/Notebooks/ch6_linear_model_selection_and_regularisation_applied.ipynb">Chapter 6 - Linear Model Selection and Regularization: Applied</a></p>
<p><a href="http://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch7_moving_beyond_linearity_labs.ipynb">Chapter 7 - Moving Beyond Linearity: Labs</a><br />
<a href="http://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch7_moving_beyond_linearity_applied.ipynb">Chapter 7 - Moving Beyond Linearity: Applied</a></p>
<p><a href="http://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch8_tree_based_methods_labs.ipynb">Chapter 8 - Tree-Based Methods: Labs</a><br />
<a href="http://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch8_tree_based_methods_conceptual.ipynb">Chapter 8 - Tree-Based Methods: Conceptual</a><br />
<a href="http://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch8_tree_based_methods_applied.ipynb">Chapter 8 - Tree-Based Methods: Applied</a></p>
<p><a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch9_support_vector_machines_labs.ipynb">Chapter 9 - Support Vetor Machines: Labs</a><br />
<a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch9_support_vector_machines_conceptual.ipynb">Chapter 9 - Support Vetor Machines: Conceptual</a><br />
<a href="https://nbviewer.jupyter.org/github/a-martyn/ISL-python/blob/master/Notebooks/ch9_support_vector_machines_applied.ipynb">Chapter 9 - Support Vetor Machines: Applied</a></p>
<p>The source code for these notebooks is available on <a href="https://github.com/a-martyn/ISL-python">Github here</a>.</p>
<h3 id="ideas-for-future-study"><a href="#ideas-for-future-study">Ideas for future study</a></h3>
<p>Below is an evolving and rough plan for areas of study and projects that I’d like to do dive into next.</p>
<h5 id="probability-theory-bayesian-statistics">Probability Theory / Bayesian Statistics</h5>
<p>I think we need some introduction to bayesian statistics. Options I’ve found so far are:</p>
<ul>
<li>Curriculum: <a href="https://www.countbayesie.com/blog/2016/5/1/a-guide-to-bayesian-statistics">Count Bayes</a></li>
<li>Lectures: <a href="https://www.youtube.com/user/elfpower/videos">Aubrey Clayton’s Logic of Science lecture series</a></li>
<li>Book: <a href="https://books.google.co.uk/books/about/Probability_Theory.html?id=tTN4HuUNXjgC&amp;source=kp_book_description&amp;redir_esc=y"><em>Probability Theory: The Logic of Science</em> by E.T. Jaynes</a>, Bayesian Statistics, them seminal work.</li>
<li>Book: <a href="https://www.amazon.com/Doing-Bayesian-Data-Analysis-Second/dp/0124058884/ref=as_li_ss_tl?ie=UTF8&amp;qid=1462141686&amp;sr=8-1&amp;keywords=doing+bayesian+data+analysis&amp;linkCode=sl1&amp;tag=counbaye09-20&amp;linkId=d4059e53b7b13b9daa785421e5bf99a5"><em>Doing Bayesian Data Analysis</em> by John K. Kruschke</a>, practical Bayesian Data Analysis</li>
</ul>
<h5 id="game-theory">Game Theory</h5>
<ul>
<li>Course: <a href="https://www.coursera.org/learn/game-theory-1">Game Theory (Coursera)</a></li>
<li>Project: submit at contribution to the <a href="https://github.com/Axelrod-Python/Axelrod">Axlerod project</a></li>
</ul>
<h5 id="reinforcement-learning">Reinforcement learning</h5>
<ul>
<li>Lectures: <a href="https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ">Introduction to Reinforcement Learning by David Silver</a></li>
<li>Book: <a href="https://www.amazon.co.uk/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249">Reinforcement Learning: An Introduction (2nd Edition)</a></li>
<li>Project: Try and solve some arcade game, or some other simulated problem</li>
</ul>
<h5 id="deep-learning">Deep learning</h5>
<ul>
<li>Course: <a href="https://www.coursera.org/specializations/deep-learning">Andrew Ng’s Deep Learning Specialisation</a></li>
<li>Book: <a href="https://www.deeplearningbook.org/"><em>Deep Learning</em> by Ian Goodfellow and Yoshua Bengio and Aaron Courville</a></li>
</ul>
<h5 id="other-references">Other references</h5>
<ul>
<li>This is the best <a href="https://humancompatible.ai/bibliography">ML bibliography</a> I’ve found from Centre for Human-Compatible Artificial Intelligence.</li>
</ul>

  </div>

  <div class="content">
    <div id="disqus_thread"></div>
  <script>

  /**
  *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
  *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

  var disqus_config = function () {
  this.page.url = alanmartyn.com/content/ml-curriculum.html;  // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = ml-curriculum; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };

  (function() { // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');
  s.src = 'https://alanmartyn.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</body>

</html>